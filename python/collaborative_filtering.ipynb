{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "CatalogException",
     "evalue": "Catalog Error: Table with name reviews does not exist!\nDid you mean \"pg_views\"?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCatalogException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mduckdb\u001b[39;00m\n\u001b[0;32m      7\u001b[0m con \u001b[39m=\u001b[39m duckdb\u001b[39m.\u001b[39mconnect(\u001b[39m'\u001b[39m\u001b[39mdatabase.db\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m podcast_ratings_query \u001b[39m=\u001b[39m con\u001b[39m.\u001b[39;49msql(\u001b[39m\"\u001b[39;49m\u001b[39mselect author_id as user_id, podcasts.title as podcast_title,rating from reviews \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m+\u001b[39;49m \n\u001b[0;32m      9\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mjoin categories using (podcast_id) \u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m\n\u001b[0;32m     10\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mjoin podcasts using (podcast_id) where average_rating >= 0\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39mto_df()\n\u001b[0;32m     12\u001b[0m podcast_titles_query \u001b[39m=\u001b[39m con\u001b[39m.\u001b[39msql(\u001b[39m\"\u001b[39m\u001b[39mselect podcasts.title as podcast_title from reviews \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m \n\u001b[0;32m     13\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mjoin categories using (podcast_id) \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m\n\u001b[0;32m     14\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mjoin podcasts using (podcast_id) where average_rating >= 0\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto_df()\n",
      "\u001b[1;31mCatalogException\u001b[0m: Catalog Error: Table with name reviews does not exist!\nDid you mean \"pg_views\"?"
     ]
    }
   ],
   "source": [
    "from typing import Dict,Text\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "import numpy as np\n",
    "import duckdb\n",
    "con = duckdb.connect('database.db')\n",
    "podcast_ratings_query = con.sql(\"select author_id as user_id, podcasts.title as podcast_title,rating from reviews \"+ \n",
    "        \"join categories using (podcast_id) \" +\n",
    "        \"join podcasts using (podcast_id) where average_rating >= 0\").to_df()\n",
    "\n",
    "podcast_titles_query = con.sql(\"select podcasts.title as podcast_title from reviews \"+ \n",
    "        \"join categories using (podcast_id) \" +\n",
    "        \"join podcasts using (podcast_id) where average_rating >= 0\").to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "podcast_ratings = podcast_ratings_query.to_dict(orient='records')\n",
    "podcast_titles = podcast_titles_query.to_dict(orient='records')\n",
    "podcast_ratings_tf = tf.data.Dataset.from_tensor_slices(pd.DataFrame.from_dict(podcast_ratings).to_dict(orient=\"list\"))\n",
    "podcast_titles_tf = tf.data.Dataset.from_tensor_slices(pd.DataFrame.from_dict(podcast_titles).to_dict(orient=\"list\"))\n",
    "\n",
    "ratings_tf = podcast_ratings_tf.map(lambda x: {\n",
    "    \"podcast_title\": x[\"podcast_title\"],\n",
    "    \"user_id\": x[\"user_id\"],\n",
    "    \"user_rating\": x[\"rating\"]\n",
    "})\n",
    "titles_tf = podcast_titles_tf.map(lambda x: x[\"podcast_title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "shuffled = ratings_tf.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(80_000)\n",
    "test = shuffled.skip(80_000).take(20_000)\n",
    "\n",
    "podcast_titles = titles_tf.batch(1_000)\n",
    "user_ids = ratings_tf.batch(1_000_000).map(lambda x: x[\"user_id\"])\n",
    "\n",
    "unique_podcast_titles = np.unique(np.concatenate(list(podcast_titles)))\n",
    "unique_user_ids = np.unique(np.concatenate(list(user_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4/4 [==============================] - 1s 66ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0026 - factorized_top_k/top_5_categorical_accuracy: 0.0026 - factorized_top_k/top_10_categorical_accuracy: 0.0143 - factorized_top_k/top_50_categorical_accuracy: 0.0558 - factorized_top_k/top_100_categorical_accuracy: 0.1649 - loss: 849.7741 - regularization_loss: 0.0000e+00 - total_loss: 849.7741       \n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 65ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0299 - factorized_top_k/top_50_categorical_accuracy: 0.1494 - factorized_top_k/top_100_categorical_accuracy: 0.3584 - loss: 788.7348 - regularization_loss: 0.0000e+00 - total_loss: 788.7348\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 65ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0039 - factorized_top_k/top_10_categorical_accuracy: 0.0390 - factorized_top_k/top_50_categorical_accuracy: 0.1896 - factorized_top_k/top_100_categorical_accuracy: 0.3935 - loss: 675.7689 - regularization_loss: 0.0000e+00 - total_loss: 675.7689\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 66ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0052 - factorized_top_k/top_10_categorical_accuracy: 0.0468 - factorized_top_k/top_50_categorical_accuracy: 0.2377 - factorized_top_k/top_100_categorical_accuracy: 0.4727 - loss: 606.0287 - regularization_loss: 0.0000e+00 - total_loss: 606.0287\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 67ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0130 - factorized_top_k/top_10_categorical_accuracy: 0.0649 - factorized_top_k/top_50_categorical_accuracy: 0.2545 - factorized_top_k/top_100_categorical_accuracy: 0.4896 - loss: 560.3953 - regularization_loss: 0.0000e+00 - total_loss: 560.3953\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 69ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0182 - factorized_top_k/top_10_categorical_accuracy: 0.0701 - factorized_top_k/top_50_categorical_accuracy: 0.2636 - factorized_top_k/top_100_categorical_accuracy: 0.4896 - loss: 533.4403 - regularization_loss: 0.0000e+00 - total_loss: 533.4403\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 65ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0026 - factorized_top_k/top_5_categorical_accuracy: 0.0299 - factorized_top_k/top_10_categorical_accuracy: 0.0818 - factorized_top_k/top_50_categorical_accuracy: 0.2649 - factorized_top_k/top_100_categorical_accuracy: 0.4896 - loss: 517.1532 - regularization_loss: 0.0000e+00 - total_loss: 517.1532\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 65ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0052 - factorized_top_k/top_5_categorical_accuracy: 0.0325 - factorized_top_k/top_10_categorical_accuracy: 0.0844 - factorized_top_k/top_50_categorical_accuracy: 0.2649 - factorized_top_k/top_100_categorical_accuracy: 0.4909 - loss: 507.1791 - regularization_loss: 0.0000e+00 - total_loss: 507.1791\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 66ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0299 - factorized_top_k/top_10_categorical_accuracy: 0.0818 - factorized_top_k/top_50_categorical_accuracy: 0.2662 - factorized_top_k/top_100_categorical_accuracy: 0.4909 - loss: 502.0438 - regularization_loss: 0.0000e+00 - total_loss: 502.0438\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 63ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0299 - factorized_top_k/top_10_categorical_accuracy: 0.0818 - factorized_top_k/top_50_categorical_accuracy: 0.2662 - factorized_top_k/top_100_categorical_accuracy: 0.4909 - loss: 499.4203 - regularization_loss: 0.0000e+00 - total_loss: 499.4203\n",
      "4/4 [==============================] - 1s 67ms/step - factorized_top_k/top_1_categorical_accuracy: 0.6325 - factorized_top_k/top_5_categorical_accuracy: 0.6494 - factorized_top_k/top_10_categorical_accuracy: 0.6701 - factorized_top_k/top_50_categorical_accuracy: 0.7221 - factorized_top_k/top_100_categorical_accuracy: 0.7857 - loss: 497.9956 - regularization_loss: 0.0000e+00 - total_loss: 497.9956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_recommenders.layers.factorized_top_k.Streaming at 0x1a6e45763b0>"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PodcastlensModel(tfrs.Model):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    embedding_dimension = 32\n",
    "    self.user_model = tf.keras.Sequential([\n",
    "      tf.keras.layers.StringLookup(\n",
    "          vocabulary=unique_user_ids, mask_token=None),\n",
    "      # We add an additional embedding to account for unknown tokens.\n",
    "      tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
    "    ])\n",
    "\n",
    "    self.podcast_model = tf.keras.Sequential([\n",
    "      tf.keras.layers.StringLookup(\n",
    "          vocabulary=unique_podcast_titles, mask_token=None),\n",
    "      tf.keras.layers.Embedding(len(unique_podcast_titles) + 1, embedding_dimension)\n",
    "    ])\n",
    "    metrics = tfrs.metrics.FactorizedTopK(\n",
    "      candidates=titles_tf.batch(128).map(self.podcast_model)\n",
    "    )\n",
    "\n",
    "    self.task = tfrs.tasks.Retrieval(\n",
    "      metrics=metrics\n",
    "    )\n",
    "\n",
    "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "    # We pick out the user features and pass them into the user model.\n",
    "    user_embeddings = self.user_model(features[\"user_id\"])\n",
    "    # And pick out the podcast features and pass them into the podcast model,\n",
    "    # getting embeddings back.\n",
    "    positive_podcast_embeddings = self.podcast_model(features[\"podcast_title\"])\n",
    "\n",
    "    # The task computes the loss and the metrics.\n",
    "    return self.task(user_embeddings, positive_podcast_embeddings)\n",
    "\n",
    "model = PodcastlensModel()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n",
    "cached_train = train.shuffle(100_000).batch(256).cache()\n",
    "\n",
    "model.fit(cached_train, epochs=10)\n",
    "\n",
    "model.evaluate(cached_train, return_dict=True)\n",
    "# Create a model that takes in raw query features, and\n",
    "index = tfrs.layers.factorized_top_k.Streaming(model.user_model)\n",
    "# recommends podcasts out of the entire podcasts dataset.\n",
    "index.index_from_dataset(\n",
    "  tf.data.Dataset.zip((titles_tf.batch(10), titles_tf.batch(10).map(model.podcast_model)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for user 0d14d171f46c19b: ['All of the Above radio', 'Rebel Chums', 'Things a Teacher Taught Me']\n",
      "\n",
      "Recommendations for user 42ded2f5c6d7ac3: ['F*ck Like a Woman', 'I Crush Barbecue Show', 'Rebel Chums', 'Things a Teacher Taught Me']\n",
      "\n",
      "Recommendations for user 91ce11be82ebaf7: ['Hear It Now (retired)', 'Inglestotal : Cursos y clases gratis de Ingles', 'Nahh B! Podcast MMA / UFC And Boxing Event Preview & Reviews', 'Twin Talk with the King Twins']\n",
      "\n",
      "Recommendations for user b3dd55b0f6dea86: ['F*ck Like a Woman', 'Nahh B! Podcast MMA / UFC And Boxing Event Preview & Reviews', 'Talking Web Marketing']\n",
      "\n",
      "Recommendations for user e49452d7f3403b3: ['Nahh B! Podcast MMA / UFC And Boxing Event Preview & Reviews', 'Talking Web Marketing', 'Twin Talk with the King Twins']\n",
      "\n",
      "Number of users with recommendations over 2: 5\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "threshold = 2\n",
    "for user in unique_user_ids:\n",
    "    user_id = user.decode()\n",
    "    _, titles = index(tf.constant([user_id]))\n",
    "    unique_preds = np.unique(titles.numpy())\n",
    "    unique_preds = [el.decode('UTF-8') for el in unique_preds]\n",
    "    if len(unique_preds) > threshold:\n",
    "        print(\"Recommendations for user {}: {}\".format(user_id,unique_preds))\n",
    "        print(\"\")\n",
    "        num += 1\n",
    "print(\"Number of users with recommendations over {}: {}\".format(threshold,num))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
